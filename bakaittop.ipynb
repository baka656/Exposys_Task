{"metadata":{"anaconda-cloud":{},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Niki.ai","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","metadata":{"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-7d00d4334a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"],"ename":"ModuleNotFoundError","evalue":"No module named 'nltk'","output_type":"error"}]},{"cell_type":"code","source":"df = pd.read_csv(\"label.txt\",sep=\",,,\",header=None ,names=['question','type'])","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n  \"\"\"Entry point for launching an IPython kernel.\n","output_type":"stream"}]},{"cell_type":"code","source":"df.head()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/html":"<div>\n<style>\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>how did serfdom develop in and then leave russ...</td>\n      <td>unknown</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>what films featured the character popeye doyle ?</td>\n      <td>what</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>how can i find a list of celebrities ' real na...</td>\n      <td>unknown</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>what fowl grabs the spotlight after the chines...</td>\n      <td>what</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>what is the full form of .com ?</td>\n      <td>what</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"                                            question      type\n0  how did serfdom develop in and then leave russ...   unknown\n1  what films featured the character popeye doyle ?       what\n2  how can i find a list of celebrities ' real na...   unknown\n3  what fowl grabs the spotlight after the chines...      what\n4                   what is the full form of .com ?       what"},"metadata":{}}]},{"cell_type":"code","source":"df.shape","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(1483, 2)"},"metadata":{}}]},{"cell_type":"code","source":"df['type']=df['type'].str.strip()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df['type'].unique()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"array(['unknown', 'what', 'when', 'who', 'affirmation'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"df['question'].values","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"array(['how did serfdom develop in and then leave russia ? ',\n       'what films featured the character popeye doyle ? ',\n       \"how can i find a list of celebrities ' real names ? \", ...,\n       'does this hose have one ? ', 'can i get it in india ? ',\n       'would this work on a 2008 ford edge with a naked roof ? '], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"df['question'] = df['question'].apply(lambda x: x.lower())\ndf['question'] = df['question'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"VALIDATION_SPLIT=0.20","metadata":{},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"## Naive Bayes","metadata":{}},{"cell_type":"code","source":"from collections import Counter, defaultdict\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pickle as pkl\nfrom sklearn.naive_bayes import MultinomialNB\n# organize imports\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import SnowballStemmer\nfrom nltk import word_tokenize\nfrom nltk.corpus import wordnet as wn","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"class StemTokenizer(object):\n    def __init__(self):\n        self.ignore_set = {'footnote', 'nietzsche', 'plato', 'mr.'}\n\n    def __call__(self, doc):\n        words = []\n        for word in word_tokenize(doc):\n            word = word.lower()\n            w = wn.morphy(word)\n            if w and len(w) > 1 and w not in self.ignore_set:\n                words.append(w)\n        return words","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"stemmer = SnowballStemmer('english').stem\ndef stem_tokenize(text):\n    return [stemmer(i) for i in word_tokenize(text)]","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"### Using Count Vectorizer ","metadata":{}},{"cell_type":"code","source":"vectorizer = CountVectorizer(analyzer='word',lowercase=True,tokenizer=stem_tokenize)\nX_train = vectorizer.fit_transform(df.question.values)\nwith open('vectorizer.pk', 'wb') as fin:\n    pkl.dump(vectorizer, fin)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"labels = data['type']","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"# split the data into a training set and a validation set\nindices = np.arange(X_train.shape[0])\nnp.random.shuffle(indices)\nX_train = X_train[indices]\nlabels = labels[indices]\nnb_validation_samples = int(VALIDATION_SPLIT * X_train.shape[0])\n\nx_train = X_train[:-nb_validation_samples]\ny_train = labels[:-nb_validation_samples]\nx_val = X_train[-nb_validation_samples:]\ny_val = labels[-nb_validation_samples:]","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"clf = MultinomialNB()\nclf.fit(x_train,y_train)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":89,"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"},"metadata":{}}]},{"cell_type":"code","source":"# evaluate the model of test data\npreds = clf.predict(x_val)\nprint(classification_report(preds,y_val))\nprint(\"Accuracy :\",clf.score(x_val,y_val))","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":90,"outputs":[{"name":"stdout","output_type":"stream","text":"             precision    recall  f1-score   support\n\naffirmation       0.61      1.00      0.76        17\n    unknown       0.72      0.85      0.78        52\n       what       0.98      0.78      0.87       139\n       when       0.33      0.75      0.46         8\n        who       0.96      0.94      0.95        80\n\navg / total       0.89      0.85      0.86       296\n\nAccuracy : 0.847972972973\n"}]},{"cell_type":"code","source":"example=vectorizer.transform([\"What time does the train leave\"])\nclf.predict(example)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":91,"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"array(['what'],\n      dtype='<U11')"},"metadata":{}}]},{"cell_type":"markdown","source":"### Using TF-IDF (though bad choice for short sequences or corpus)","metadata":{}},{"cell_type":"code","source":"tf_vectorizer = TfidfVectorizer(analyzer='word',lowercase=True,tokenizer=stem_tokenize)\nX_train = tf_vectorizer.fit_transform(df.question.values)\nwith open('tf_vectorizer.pk', 'wb') as fin:\n    pkl.dump(tf_vectorizer, fin)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"labels = data['type']\n# split the data into a training set and a validation set\nindices = np.arange(X_train.shape[0])\nnp.random.shuffle(indices)\nX_train = X_train[indices]\nlabels = labels[indices]\nnb_validation_samples = int(VALIDATION_SPLIT * X_train.shape[0])\n\nx_train = X_train[:-nb_validation_samples]\ny_train = labels[:-nb_validation_samples]\nx_val = X_train[-nb_validation_samples:]\ny_val = labels[-nb_validation_samples:]","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"clf = MultinomialNB()\nclf.fit(x_train,y_train)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":94,"outputs":[{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"},"metadata":{}}]},{"cell_type":"code","source":"# evaluate the model of test data\npreds = clf.predict(x_val)\nprint(classification_report(preds,y_val))\nprint(\"Accuracy :\",clf.score(x_val,y_val))","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":95,"outputs":[{"name":"stdout","output_type":"stream","text":"             precision    recall  f1-score   support\n\naffirmation       0.10      1.00      0.18         2\n    unknown       0.45      1.00      0.62        32\n       what       1.00      0.56      0.72       194\n       when       0.00      0.00      0.00         0\n        who       0.80      0.88      0.84        68\n\navg / total       0.89      0.69      0.73       296\n\nAccuracy : 0.685810810811\n"},{"name":"stderr","output_type":"stream","text":"C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n  'recall', 'true', average, warn_for)\n"}]},{"cell_type":"code","source":"example=tf_vectorizer.transform([\"What time does the train leave\"])\nclf.predict(example)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":96,"outputs":[{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"array(['what'],\n      dtype='<U11')"},"metadata":{}}]},{"cell_type":"markdown","source":"## LSTM","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"MAX_NB_WORDS = 20000\nMAX_SEQUENCE_LENGTH=30","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport re","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"data=df.copy()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"print(data['type'].value_counts())\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, split=' ')\ntokenizer.fit_on_texts(data['question'].values)\nX = tokenizer.texts_to_sequences(data['question'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":22,"outputs":[{"name":"stdout","output_type":"stream","text":"what           609\nwho            402\nunknown        272\naffirmation    104\nwhen            96\nName: type, dtype: int64\n"},{"name":"stderr","output_type":"stream","text":"C:\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\text.py:145: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"}]},{"cell_type":"code","source":"word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\nY = data['type']\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(Y)\nY=le.transform(Y) \nlabels = to_categorical(np.asarray(Y))\nprint('Shape of data tensor:', X.shape)\nprint('Shape of label tensor:', labels.shape)\n\n\n# split the data into a training set and a validation set\nindices = np.arange(X.shape[0])\nnp.random.shuffle(indices)\nX = X[indices]\nlabels = labels[indices]\nnb_validation_samples = int(VALIDATION_SPLIT * X.shape[0])\n\nx_train = X[:-nb_validation_samples]\ny_train = labels[:-nb_validation_samples]\nx_val = X[-nb_validation_samples:]\ny_val = labels[-nb_validation_samples:]","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":75,"outputs":[{"name":"stdout","output_type":"stream","text":"Found 3685 unique tokens.\nShape of data tensor: (1483, 30)\nShape of label tensor: (1483, 5)\n"}]},{"cell_type":"code","source":"embeddings_index = {}\nf = open('E:/Projects/Word2Vec/glove.42B.300d.txt', encoding=\"utf8\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":26,"outputs":[{"name":"stdout","output_type":"stream","text":"Found 1917495 word vectors.\n"}]},{"cell_type":"code","source":"EMBEDDING_DIM=300","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Embedding\n\nembedding_layer = Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"embed_dim = 300\nlstm_out = 196\n\nmodel = Sequential()\nmodel.add(embedding_layer)\nmodel.add(LSTM(lstm_out, dropout_U=0.25, dropout_W=0.25))\nmodel.add(Dense(5,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":32,"outputs":[{"name":"stderr","output_type":"stream","text":"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, dropout=0.25, recurrent_dropout=0.25)`\n"},{"name":"stdout","output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 30, 300)           1105800   \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 196)               389648    \n_________________________________________________________________\ndense_2 (Dense)              (None, 5)                 985       \n=================================================================\nTotal params: 1,496,433\nTrainable params: 390,633\nNon-trainable params: 1,105,800\n_________________________________________________________________\nNone\n"}]},{"cell_type":"code","source":"model.fit(x_train, y_train,\n          batch_size=128,\n          epochs=20,\n          validation_data=(x_val, y_val))","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":33,"outputs":[{"name":"stdout","output_type":"stream","text":"Train on 1187 samples, validate on 296 samples\nEpoch 1/20\n1187/1187 [==============================] - 1s - loss: 1.3851 - acc: 0.4229 - val_loss: 1.2064 - val_acc: 0.5203\nEpoch 2/20\n1187/1187 [==============================] - 0s - loss: 1.0770 - acc: 0.6091 - val_loss: 0.8535 - val_acc: 0.6723\nEpoch 3/20\n1187/1187 [==============================] - 0s - loss: 0.6710 - acc: 0.7498 - val_loss: 0.6229 - val_acc: 0.7939\nEpoch 4/20\n1187/1187 [==============================] - 0s - loss: 0.4351 - acc: 0.8585 - val_loss: 0.4273 - val_acc: 0.8649\nEpoch 5/20\n1187/1187 [==============================] - 0s - loss: 0.3035 - acc: 0.9149 - val_loss: 0.3121 - val_acc: 0.9088\nEpoch 6/20\n1187/1187 [==============================] - 0s - loss: 0.2376 - acc: 0.9360 - val_loss: 0.2819 - val_acc: 0.9257\nEpoch 7/20\n1187/1187 [==============================] - 0s - loss: 0.1970 - acc: 0.9461 - val_loss: 0.1894 - val_acc: 0.9459\nEpoch 8/20\n1187/1187 [==============================] - 0s - loss: 0.1411 - acc: 0.9621 - val_loss: 0.1626 - val_acc: 0.9527\nEpoch 9/20\n1187/1187 [==============================] - 0s - loss: 0.1110 - acc: 0.9747 - val_loss: 0.1830 - val_acc: 0.9527\nEpoch 10/20\n1187/1187 [==============================] - 0s - loss: 0.1142 - acc: 0.9730 - val_loss: 0.1336 - val_acc: 0.9662\nEpoch 11/20\n1187/1187 [==============================] - 0s - loss: 0.0940 - acc: 0.9789 - val_loss: 0.1373 - val_acc: 0.9730\nEpoch 12/20\n1187/1187 [==============================] - 0s - loss: 0.0852 - acc: 0.9798 - val_loss: 0.1156 - val_acc: 0.9764\nEpoch 13/20\n1187/1187 [==============================] - 0s - loss: 0.0744 - acc: 0.9823 - val_loss: 0.1117 - val_acc: 0.9797\nEpoch 14/20\n1187/1187 [==============================] - 0s - loss: 0.0643 - acc: 0.9848 - val_loss: 0.1110 - val_acc: 0.9797\nEpoch 15/20\n1187/1187 [==============================] - 0s - loss: 0.0578 - acc: 0.9857 - val_loss: 0.1078 - val_acc: 0.9797\nEpoch 16/20\n1187/1187 [==============================] - 0s - loss: 0.0532 - acc: 0.9882 - val_loss: 0.1042 - val_acc: 0.9797\nEpoch 17/20\n1187/1187 [==============================] - 0s - loss: 0.0527 - acc: 0.9840 - val_loss: 0.1258 - val_acc: 0.9696\nEpoch 18/20\n1187/1187 [==============================] - 0s - loss: 0.0547 - acc: 0.9815 - val_loss: 0.1056 - val_acc: 0.9730\nEpoch 19/20\n1187/1187 [==============================] - 0s - loss: 0.0440 - acc: 0.9907 - val_loss: 0.1253 - val_acc: 0.9662\nEpoch 20/20\n1187/1187 [==============================] - 0s - loss: 0.0360 - acc: 0.9907 - val_loss: 0.1204 - val_acc: 0.9764\n"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x258b850f2e8>"},"metadata":{}}]},{"cell_type":"code","source":"example = tokenizer.texts_to_sequences([\"What time does the flight leave\"])\nexample = pad_sequences(example, maxlen=MAX_SEQUENCE_LENGTH)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"le.inverse_transform(np.argmax(model.predict(example)))","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"'when'"},"metadata":{}}]}]}